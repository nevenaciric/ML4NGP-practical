{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pre-prepared dataset of disordered and ordered protein regions using the <code>pandas</code> package. Function <code>read_csv</code> reads from file in CSV (Comma-Separated Values) format and stores the data into the <code>DataFrame</code> object. <code>DataFrame</code> is a <code>pandas</code> 2-dimensional data structure that is used for storing tabular data, i.e. data that is organized in a table with rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting column <code>'ID'</code> as the index of the <code>DataFrame</code> object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions of the <code>DataFrame</code> object, i.e. the size of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset into a training and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is split into a training and test subsets in ratio 2:1. The splitting of the dataset is done using the stratification technique (parameter <code>stratify</code>) that preserves the same proportions of instances in each class as observed in the original dataset. The value of the parameter <code>random_state</code> is fixed in order to make reproducible train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram sequence representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that certain amino acids and their combinations are more likely (i.e. frequent) in ordered than disordered protein regions<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1), n-gram representation of protein sequences come as a natural choice. Based on the n-gram composition, different representations of protein sequences, suitable for machine learning models, can be constructed. Two basic methods, inherited from the field of Natural Language Processing, are:\n",
    "- *Bag of Words* or *Bag of n-grams* method\n",
    "- *TF-IDF* (Term Frequency - Inverse Document Frequency) method \n",
    "\n",
    "We can easily generate these two types of n-gram-based sequence representation using the <code>feature_extraction</code> module (<code>text</code> submodule) from <code>sklearn</code> package.\n",
    "\n",
    "<a name=\"cite_note-1\"></a>[<sup>[1]</sup>](#cite_ref-1)Hydrophobic amino acids, more precisely clusters of hydrophobic amino acids are characteristical for ordered regions of proteins, while the hydrophilic amino acids are more prevalent in disordered regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we well try with Bag of Words representations and train several types of classification models. Scikit-learn library provides support for n-gram sequence representation using the *Bag of Words method*  through the <code>CountVectorizer</code> class. Constructor function has various settings, mainly related to the preprocessing phase:\n",
    "- <code>analyzer</code> - whether to tokenize text by the words or individual characters \n",
    "- <code>ngram_range</code> - the lower and upper boundary of the range of char n-grams to be extracted, only applies if analyzer is set to 'char'; e.g. an n-gram range of (1, 3) means that only unigrams, bigrams and trigrams will be extracted\n",
    "- <code>min_df</code> - lower cutoff value of the frequency values, used for excluding tokens that have a frequency less than a given threshold \n",
    "- <code>max_df</code> - upper cutoff value of the frequency values, used for excluding tokens that have a frequency higher than a given threshold \n",
    "- <code>lowercase</code> - converting all characters to lowercase before tokenizing\n",
    "- <code>stop_words</code> - excluding common words or words that are considered irrelevant for the specific problem\n",
    "- <code>token_pattern</code> - excluding words that do not match a predefined format \n",
    "- <code>tokenizer</code> i <code>preprocessor</code> - passing a custom function to perform tokenization/preprocessing \n",
    "- <code>vocabulary</code> - assigning a pre-prepared vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set only the parameters <code>analyzer</code>, <code>ngram_range</code> and <code>min_df</code>, while default values will be taken for other parameters. We want to split text (protein sequences) by individual characters while extracting only the unigrams, bigrams and trigrams. Also we will want to exclude n-grams that appear less than 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary<a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2) is, by rule, built over a training set. Later, while obtaining vector representations for test set instances, all unknown tokens (n-grams that are not present in the vocabulary) will be ignored.\n",
    "\n",
    "<a name=\"cite_note-2\"></a>[<sup>[2]</sup>](#cite_ref-2)Set of features (words or in our case n-grams) which will be used for sequence representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of extracted features (n-grams) can be obtained by method <code>get_feature_names()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of extracted features (n-grams):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization of training and test set sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a simple logistic regression model and train it on a vectorized training set. Scikit-learn library provides support for different linear models, as well for the logistic regression model, through <code>linear_model</code> package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model on the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a linear SVM (Support Vector Machine) model and train it on a vectorized training set. Support for linear SVM model is provided through the <code>LinearSVC</code> class from the <code>svm</code> module of the <code>sklearn</code> package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model on the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - k nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a *k* nearest neighbors classifier (observing the 4 nearest neighbors) and train it on a vectorized training set. Support for KNN model is provided through the <code>KNeighborsClassifier</code> class from the <code>neighbors</code> module of the <code>sklearn</code> package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model on the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we well try with TF-IDF representations and train same types of classification models in order to compare them with the previous ones. For obtaining the TF-IDF representations of sequences we will use the <code>TfidfVectorizer</code> class from module <code>sklearn.text.feature_extraction</code>. The set of parameters when calling a constructor function is the same as for the <code>CountVectorzer</code> class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via parameters <code>analyzer</code> and <code>ngram_range</code> we set text (protein sequences) to be tokenized into characters (instead of words, which is default) and n-gram range which is going to extracted. Also, we exclude tokens (n-grams) that appear less than 5 times (<code>min_df</code> parameter) and chose to use only the \"TF part\" of the TF-IDF metric bu setting the parameter <code>use_idf</code> to <code>False</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building vocabulary based on instances of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of extracted features (n-grams) that make up the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of extracted features (n-grams):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization of training and test set sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a simple logistic regression model and train it on the now TF-IDF vectorized training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model on the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a linear SVM (Support Vector Machine) model and train it on the now TF-IDF vectorized training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model on the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - k nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a k nearest neighbors classifier (observing the 4 nearest neighbors) and train it on the now TF-IDF vectorized training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model on the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of constructed models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection of the most relevant n-grams for disorder-order classification (i.e. model interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function visualizes the coefficients of the model <code>classifier</code>, i.e. the corresponding features (n-grams) <code>feature_names</code> showing only the <code>n_top_features</code> features (n-grams) that are most relevant for predicting the negative (disorder) and positive (order) classes. For the title of the graph will be set <code>title</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coefficients(title, classifier, feature_names, n_top_features=25):\n",
    "    coefs = classifier.coef_.ravel()\n",
    "    \n",
    "    negative_coefs_indices = np.argsort(coefs)[:n_top_features]\n",
    "    positive_coefs_indices = np.argsort(coefs)[-n_top_features:]\n",
    "    \n",
    "    most_decisive_coefs_indices = np.hstack([negative_coefs_indices, positive_coefs_indices])\n",
    "    most_decisive_coefs = coefs[most_decisive_coefs_indices]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title(title)\n",
    "    colors = ['orange' if c < 0 else 'cadetblue' for c in most_decisive_coefs]\n",
    "    plt.bar(np.arange(2*n_top_features), most_decisive_coefs, color=colors)\n",
    "    \n",
    "    most_decisive_feature_names = np.array(feature_names)[most_decisive_coefs_indices]\n",
    "    plt.xticks(np.arange(2*n_top_features), most_decisive_feature_names, rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model k nearest neighbors does not have an explicit form, that is, it is not determined by some coefficients, so we cannot get to the most relevant words in this way.\n",
    "\n",
    "This analysis can as well be performed for models trained on Bag of Words representations of protein sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the classification accuracy of all 6 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/comparing_models.png\" width=750 align=\"left\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
